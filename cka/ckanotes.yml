CKA exam distribution

Cluster Architecture, Installation & Configuration	25%
Workloads & Scheduling	15%
Services & Networking	20%
Storage	10%
Troubleshooting	30%


Nodes
  Control plain / Master Node
  Worker Nodes

  Control plain / Master Nodes components
    etcd - cluster info datastore
    scheduler - logic for which node a pod is created
    control manager - (node controller, Replication controller) make sure that desired state is achieved
    api server - rest api server for all components - orchestrating all components

  Worker Nodes
    kubelet - recieves the command from scheduler to perform the action (create node, destroy, status, etc)
    kubeproxy - worker nodes networking

etcd
  a key value store
  etcdctl cli
  default port 2379
  installed manually  
    will install as a service (etcd.service) on the node - /usr/local/bin/etcd
    multiple master node will have multiple etcd
      make sure etcd is aware of each other on the etcd.service
      --initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380 //
  installed via kubeadm
    creates a pod for etcd
    on kube-system namespace
    run etcdctl on the pod
      kubectl exec <podname> -n <namespace> -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 

API-Server
  Authenticate user > Validate request > retrieve data from etcd > Update ETCD > Scheduler > kubelet
  API server processes request from kubectl or RestAPI calls
  Authorizations are via certificates

  Where is the API server? Where to see the API server options?
    if installed via Kubeadm
      creates a pod on kube-system namespace on the master node
      file definition is at etc/kubernetes/manifests/kube-apiserver.yaml
    if installed manually
      on the master node
      service definition is at etc/systemd/system/kube-apiserver.service
      also see running process - ps -aux | grep kube-apiserver

Kube-controller manager
  Manages various controllers
  Controller is a process - continiously watches the status and remediate issues
    Node Controller
      checks Nodes status every 5 seconds with 40 seconds grace period before it marks as unreachable
      once marked as unreachable, the node is given 5 minutes to come back up (POD eviction timeout), else it removes the pods
    Replication-Controller
      takes care of the replicaset

  Where is the controller?
    All controllers are packaged in Kube-Controller-Manager
      Kubeadm installation
        Creates a POD kube-controller-manager in the kube-system namespace on the master node
        definition is on /etc/kubernetes/manifests/kube-controller-manager.yaml
      Manual installation
        /etc/systemd/system/kube-controller-manager.service
        ps -aux | grep kube-controller-manager

Kube-scheduler
    Decides which pod goes to which node
  
kubelet
  installed on the worker nodes
  Registers a node, Create PODS, Monitor node and Pods

  How it is installed
    Using Kubeadm
      kubelet is not automatically installed
      must be manually installed
      ps -aux | grep kubelet  on the worker node
    Manual installation
      same as above

kube-proxy
  a process that runs on each nodes
  looks for new kube services, then creates appropriate rules on each node to route traffic
  creates an IP tables rule

  How it is installed
    Kubeadm
      creates a pod on kube-system
      it is deployed as a daemonset

PODS




YAML configuration file
  Required fields in the structure
    apiVersion
    kind
    metadata   (in the form of dictionary)
    spec

    apiVersion: (root)
    kind:       (root)
    metadata:   (root) (dictionary)
      name:     (children/siblings)
      labels:   (children/siblings) - labels can have any key-value pair as you wish
        app: value
        type: value
    spec:       (root) (dictionary)
      containers: (list/array)
        - name: nginx-container ( - indicates the 1st in the list/array)
          image: nginx
        
  you can create a yaml from kubectl run by
    kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml
  edit dynamically a pod configuration
    kubectl edit pod <podname>
    editing an existing pod is only possible for image, activeDeadlineSeconds and tolerations
    if required, you can still do edit but, it will be forbidden and a temp yaml will be provided, just delete old pod and use new yaml
    kubectl create -f /tmp/kubectl-edit-ccvrq.yaml

  get pod definition
    kubectl get pod webapp -o yaml > my-new-pod.yaml


Controllers
Replication controller VS ReplicaSet
  Replica of pods - for issues and load balancing
  Replication Controller is different from ReplicaSet
  ReplicationController is the older and ReplicaSet is the newer

Example of replicationcontroller

kind: apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp
spec:
  replicas: <Replicas>
  selector:
    app: myapp
  template:
    metadata:
      name: myapp
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: <Image>
          ports:
            - containerPort: <Port>
replicas: 3

  kubectl create -f <replicationcontrollerfile>
  kubectl get repicationcontroller

Example for ReplicaSet

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels: 
    app: myapp
    tier: frontend   (this should match the selector metadata labels tier)
spec:
  template:
    metadata:
      name: myapp-pod
      labels: 
        app: myapp
        tier: frontend    (this should match the metadata labels tier)
    spec:
      containers:
      - name: nginx-container
        image: nginx
replicas: 3
selector:       (this would allow the ReplicaSet to control the other pods that are existing already)
  matchLabels:
    type: front-end

  kubectl create -f replicaset-definition.yaml
  kubectl get replicaset
  
  Labels and selectors allows the replicaset to identify which existing pods to monitor and control

scaling replicaset
  1. change the number of replicas in the definition then run the replace command
    kubectl replace -f <name of updated replicaset yaml file>
  2. run the command
    kubectl scale --replicas=<value> -f <name of yaml definition>  (this does not update the value in the file)
    kubectl scale --replicas=<value> replicaset <name of replicaset>    

  kubectl delete replicaset <replicateset name> (this deletes the pods as well)
  kubectl get all (gets all the resources)

Deployments
  Deployments > Replicaset > Pods
  Deployments automatically creates ReplicaSet which then creates the pods

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
c:
  selector:
    matchLabels:
      app: myapp
  e:
    metadata:
      labels:
         myapp
    spec:
      containers:
      - name: myapp
        image: <Image>
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: <Port>

namespace
  logical separation 
  
  DNS resolution Example
  mysql.connect("db-service")  - same namespace
  mysql.connect("db-service.dev.svc.cluster.local")   - different namespace, cluster.local is the domain of the cluster, svc is the subdomain for the service, dev is the namespace
  
  namespace created at setup
  default, kube-system, public

to createa  namespace
apiVersion: v1
kind: Namespace
metadata:
  name: dev
or
kubectl create namespace dev

to use namespace
  kubectl create -f pod-definition.yaml --namespace=dev
  or
  add in metadata
  namespace: dev
  after the name

  kubectl get pods --namespace=dev

  to change namespace context
  kubectl config set-context $(kubectl config current-context) --namespace=dev

  kubectl get pods --all-namespace
  

Resource Quota
  To limit the resource of a namespace

apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev

spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memroy: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi


Services
  Types: NodePort, ClusterIP, LoadBalancer
  Service identifies pods using the pods labels (Service > Selector is the same as Pods > Labels)
  Node port range is between 30000 - 32767

  Algorithm: random
  SessionAfinity: Yes

  NodePort Service - creates a Service that spans accross all nodes. This will make the port for any node targeted to the pod
  So, if the pod exists on any Node, you can still reach the pod NodeID:port

apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - port: 80    (port of the service object)
    targetPort: 80   (port of the target pod)
    nodePort: 30008   (port of the node)
  selector:
    app: myapp
    type: front-end

curl http://nodeip:nodeport

  ClusterIP Service - provides correct way of connecting cluster of pods (group of Frontend pods - group of Backend pods -
  group of  DB pods)

  apiVersion: v1
  kind: Service
  metadata:
    name: backend
  spec:
    type: ClusterIP
    selector:
      app: myapp
      type: back-end
    ports:
    - port: 80
      targetPort: 80
  
  LoadBalancer Service - creates a single point of access to the pods/service. Single URL will distribute the requests
  to the different service. Only works on supported Cloud platforms, else it will just work like a regular nodeport service.

  apiVersion: v1
  kind: Service
  metadata:
    name: myapp
  spec:
    type: LoadBalancer
    selector:
      app: myapp
    ports:
    - port: <Port>
      targetPort: <Target Port>
      nodePort: 30008
  


Imperative vs Declarative
  infra as a code approaches
  imperative = what to do, how to do, step by step
  declative = just declare the final state, the system will figure out how

imperative commands
  create objects
    kubectl create
    kubectl run
    kubectl expose
  update objects
    kubectl edit
    kubectl scale
    kubectl set
declarative - yaml files
    kubectl apply -f definition file


quick imperative commands, helpful in exam
create pod and service to expose the pod
k run httpd --image=httpd:alpine --port 80 --expose
create deployment with number of replicas
k create deployment redis-deploy -n dev-ns --image=redis --replicas=2
create pod then expose the pod port
k run redis --image=redis:alpine --labels=tier=db
k expose pod redis --name redis-service --port 6379 --target-port 6379

