Networking basics in Linux
Switch and Router in Linux
  ip link  - to see the interface of the host
  ip addr add 192.168.1.10/24 dev eth0
  route  - shows that route table
  ip route add default via 192.168.2.1 (any network or address, use the default gateway)

  for box with 2 eth - packets cannot be forwarded between eth
  this is controlled via /proc/sys/net/ipv4/ip_forward

DNS in Linux
  /etc/hosts
  cat >> /etc/hosts

  specify the DNS sever on
  /etc/resolv.conf
  namesever <IP address of DNS Server>

  the order at which the host look up the nameresolution can be cofigured
  /etc/nsswitch.conf

  /etc/resolv.conf
  search mycompany.com   #if you go to web it will append mycompany.com making the address web.mycompany.com

  Record types:
  A record - map IP to host name
  AAAA record - ipv6
  CNAME - name to name mapping

  Tools:
  ping www.google.com
  nslookup www.google.com #cannot see nameserver defined in /etc/hosts
  dig www.google.com #more detailed info on nameresolution

  CoreDNS
  install via 
  wget https://github.com/coredns
  listen at port 53
  CoreDNS IP-hostname mapping can be configured in /etc/hosts then confgure CoreDNS to use this file
  via the Corefile



Network Namespaces
  Used by containers 
  container will have its own network interface, routing and arp table
  
  to create network namespaces in Linux
  ip netns add red
  
  ip netns  (will lists the net namespaces)
  ip link #lists the network interface on host
  ip netns exec red ip link #lists the network interface on container
  ip -n red link #same as above

  arp #will list the arp table on the host
  ip netns exec red arp #list arp table on container
  route
  ip netns exec red route

  link container together
  ip link add veth-red type veth peer name veth-blue #create virtual link for the 2 network namespace
  ip link set veth-red netns red #associate the netns to the container
  ip link set veth-blue netns blue #associate the netns to the container

  ip -n red addr add 192.168.15.1 dev veth-red #assign an IP to the network namespace interface
  ip -n blue addr add 192.168.15.2 dev veth-blue #assign an IP to the network namespace interface

  ip -n red link set veth-red up #bring up link 
  ip -n red link set veth-red up #bring up link

  if more containers/namespace - you need a virtual Switch/virtual network
  Linux Bridge or OpenvSwitch
  ip link add v-net-0 type bridge

  ip link
  v-net-0 is now an interface of the host
  assigning an IP to v-net-0 from the host cidr, means this will not be reachable by the host

  but namespace network is only limited to the host
  it cannot reach or reachable by outside of the host
  the host now becomes a gateway
  to reach outside from namespace, ip tables needs a NAT
  iptables -t nat -A POSTROUTING -s <CIDR of namespace> -j MASQUERADE
  #this will mask the from IP of namespace to the host IP
  #nat is used as well for outsiede world to reach the namespaces but with port


Docker networking
  Docker run --network none nginx #not attached to any network
  Docker run --network host nginx #http://localhostip:80 reachable from host, only 1 can use the port
  Docker run nginx #bridge network 

  when docker is installed it creates a virtual network named bridge
  docker creates a port mapping to map external access to the container

CNI - Container Networking Interface
  All network namespace solutions are combined to a single solution named CNI
  #plugins are bridge, VLAN, IPVLAN, MACVLAN,
  bridge add <containerid> /var/run/netns/container
  #adds the container to the bridge network


Networking Cluster Nodes
  Nodes ports that needs to be open
  kube-apiserver 6443
  kubelet 10250
  kube-scheduler 10251
  kube-controller-manager 10252
  services 30000-32767
  etcd 2379
  etcd (multi etcd) 2380 
  
  netstat -anp | grep etcd
  #list the network connection statistics

Pod networking
  Pod networking are handled by a networking solution
  goverened by CNI - Container network orchestration

  ip link 
  ip -a 
  
  /opt/cni/bin - location of CNI plugins available
  /etc/cni/net.d/*.conf - location of config file specifying which plugin is being Used

  the CNI plugin - Ex. Weavenet installs an agent on each node
  that creates a bridge networking between pods
  creating a virtual network 
  
  ps -aux | grep weave #shows the running processes of the CNI plugin

Service Networking
  the pod IP is accessible only on Node
  while Service IP is accessible to the cluster
  this type of service is known as
  ClusterIP

  if service needs to be accessible from outside of cluster (web app)
  the type of service needed is
  NodePort
  Nodeport also exposes the port number

  While pods have IP assigned by CNI invoked by kubelet
  Service is invoked by kube-proxy and is cluster wide object, not tied to a specific node, not a service or namespace
  when service is created, it gets an IP address from a pre-defined range
  the kube-proxy gets the that IP address and creates a Forwarding rule on each node
  the forwarding rule will forward the traffic from the service IP:port to the pod IP:port
  by default it uses iptables, but mode can be changed
  kube-proxy --proxy-mode [iptables | userspace | ipvs]

  the clusterIP range is defined in the kube-apiserver
  ps aux | grep kube-api-server
  and can be specified by
  kube-api-server --service-cluster-ip-range ipNet (default 10.0.0.0/24)
  the pod CIDR should not overlap the Service CIDR

  you can see the rules of iptables created by kubeproxy on
  iptables -L -t nat | grep db-service

  the kube-proxy changes are also logged on
  cat /var/log/kube-proxy.log

  ip a | grep eth0  #to determine the IP/range of the node
  ip a | grep weave #to deterine the IP/range of the pods  
    kubectl logs <name of cni plugin pod> #logs of CNI provider also gives the alloc range
  ps aux | grep --service-cluster #to determine the IP range of the cluster/service
    cat /etc/kubernetes/manifests/kube-apiserver.yaml #if apiserver is a pod, the definition of the pod under commands also shows the cluster ip range
  kubectl logs <kubeproxy-pod> #logs shows which proxy mode used by kube-proxy


DNS in kubernetes
  only within the kubernetes objects, not of the nodes
  the DNS in kubernetes maps the IP of the service to the FQDN
  servicename.namespace.objecttype.root
  webapp-service.default.svc.cluster.local
  db-service.prod.svc.cluster.local
  pods can also be recorded in the kube DNS if defined explicitly, the name is derived from IP replacing dots with dashes
  10-15-0-1.prod.pod.cluster.local

CoreDNS in Kubernetes
  cat /etc/resolv.conf is where the nameserver specified

  CoreDNS are deployed as a POD from ReplicaSet (deployment)
  the CoreDNS configuration file
  cat /etc/coredns/Corefile
  this configuration file is created as a ConfigMap
  kubectl get cm
  kubectl describe cm coredns

  as the CoreDNS is deployed, it also creates a kubernetes service
  called kube-dns by default
  the kube-dns IP is then added by kubelet to the /etc/resolv.conf of the pods, to point them to this nameserver

Ingress in Kubernetes
  Services VS Ingress
  Ingress is like a layer 7 load balancer
  it load balance and also redirect to specific service depending on the URL path
  like a cloud native load balancer

  How is Ingress deployed?
  Ingress is deployed as an Ingress Controller
  the ingress solutions available are
  GCE, Nginx, Contour, Istio, haproxy, traefic
  this are deployed as another deployment in kubernetes
  nginx-ingress-deployment.yaml #to deploy the actual pod ingress controller
  nginx-ingress-resource.yaml #contains the actual rules
    kubectl get ingress
  nginx-ingress-service.yaml #NodePort service to map the service to the ingress
  nginx-ingress-configmap.yaml #for the configuration file
  nginx-ingress-serviceAccount.yaml #for the authentication

  kind: apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: myingress
    labels:
      name: myingress
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /        #this was needed when I created another ingress on another namespace
        nginx.ingress.kubernetes.io/ssl-redirect: "false"
  spec:
    rules:
    - host: wear.mywebsite.com  #this is for filtering with subdomain wear
      http:
        paths:
        - path: "/wear"         #this is for fitering with url path
          backend:
            service:
              name: wear-service
              port: 
                number: 80
        - path: "/watch"
          backend:
            service:
              name: watch-service
              port: 
                number: 8080


  practice:
  create namespace
  create configmap
  create serviceaccount
  deploy ingress controller (pod)
  create a NodePort service with selector name as the label of deploy controller
  create a ingress resource - the actual filter (ingress object)
  and make sure that annotations for rewrite and ssl is added on ingress resource








