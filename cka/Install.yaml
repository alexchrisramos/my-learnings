Design a Kubernetes cluster
  Purpose?
  Cloud or OnPrem?
  Workloads?

  kubeadm for onprem
  GKE for GCP
  Kops for AWS
  AKS for Azure

  Turnkey solutions (you configure and manage your own cluster VM's)
  OpenShift, CloudFoundry, VMware Cloud PKS, Vagrant

  Hosted Solutions / Managed Solution
  AKS, GKE, EKS, OpenShift Online

HA - High Availability
  Multiple Master/ControlPlane
  
  With multi master node
  https://master1:6443 / https://master2:6443
  these 2 nodes should have a load balancer in front
  with multiple scheduler/controller-manager
  there should be a leader election process
    kube-controller-manager --leader-elect true
        --leader-elect-lease-duration 15s #lock duration of leader
        --leader-elect-renew-deadline 10s #time to renew the lease
        --leader-elect-retry-period 2s #tries to become the leader
    same with scheduler
  
  Stacked Topology
  ETCD and controlplane components are on same nodes
  External ETCD Topology
  ETCD is separated from controlplane components

  cat /etc/systemd/system/kube-apiserver.service
  shows the configuration of kube-apiserver service
  like, address of etcd


  ETCD in HA
    ETCD is key-value store
    Read from etcd has no issue, can read from any etcd node
    ETCD also elects a leader
    all writes are performed on one node, the leader
    then sends a copy to the other nodes

    How ETCD elects a leader
    RAFT election process

    How ETCD considers a write to be successful
    if some nodes are down, as long as the Majority/quorum is good, write is successful
    Quorum = N/2+1

    Odd number of etcd instances is preffered
    and minimum recommended number of instances is 3 as it gives 1 on fault tolerance
    3/2 + 1 = 2 (rounded) > which means 1 is the fault tolerance 2 is the quorum


  etcd.service
    shows the configuration of the etcd, like peer

  

Installing with kubeadm

Master                Node1                   Node2
kube-apiserver        kubelet                 kubelet
etcd                  containerruntime        containerruntime
node controller
replicate controller


Steps
  1. should have multiple VM
  2. install docker on all nodes
  3. install kubeadm tool
  4. intialize master server
  5. configure pod network
  6. joined worker nodes
  

Kubeadm
Follow guide on 
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/



#after prerequisites, install kubeadm, kubectl and kubelet
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet=1.21.0-00 kubeadm=1.21.0-00 kubectl=1.21.0-00
sudo apt-mark hold kubelet kubeadm kubectl

#initialize the controlplane
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 10.2.223.3 --pod-network-cidr=10.244.0.0/16

#join the nodes using the token in the output
#or generate
kubeadm token create --print-join-command


#install network plugin
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

